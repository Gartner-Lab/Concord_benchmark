{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation using Concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import concord as ccd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import font_manager, rcParams\n",
    "custom_rc = {\n",
    "    'font.family': 'Arial',  # Set the desired font for this plot\n",
    "}\n",
    "\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "custom_rc = {\n",
    "    'font.family': 'Arial',  # Set the desired font for this plot\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jul29-2232'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_name = \"simulation_result_integration\"\n",
    "save_dir = f\"../save/dev_{proj_name}-{time.strftime('%b%d')}/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = f\"../data/{proj_name}/\"\n",
    "data_dir = Path(data_dir)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 0\n",
    "ccd.ul.set_seed(seed)\n",
    "\n",
    "file_suffix = f\"{time.strftime('%b%d-%H%M')}\"\n",
    "file_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = {\n",
    "    'cluster_fullcover_batchratio_1': '../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_1.pkl',\n",
    "    'cluster_fullcover_batchratio_5': '../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_5.pkl',\n",
    "    'cluster_fullcover_batchratio_20': '../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_20.pkl',\n",
    "    'cluster_partialcover_batchnum_2': '../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_2_batches.pkl',\n",
    "    'cluster_partialcover_batchnum_10': '../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_10_batches.pkl',\n",
    "    'cluster_partialcover_batchnum_30': '../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_30_batches.pkl',\n",
    "    'trajectory_full': '../save/dev_simulation_trajectory-Jul24/benchmarks_out/benchmark_results_Jul24-2238.pkl',\n",
    "    'trajectory_partial': '../save/dev_simulation_trajectory_partial-Jul24/benchmarks_out/benchmark_results_Jul24-2239.pkl',\n",
    "    'trajectory_connected' : '../save/dev_simulation_trajectory_connected-Jul24/benchmarks_out/benchmark_results_Jul24-2301.pkl',\n",
    "    'trajectory_varybatch' : '../save/dev_simulation_trajectory_varybatch-Jul29/benchmarks_out/benchmark_results_Jul29-2018.pkl',\n",
    "    'loop_full' : '../save/dev_simulation_oneloop_full-Jul25/benchmarks_out/benchmark_results_Jul25-1450.pkl',\n",
    "    'loop_partial' : '../save/dev_simulation_oneloop_partial-Jul25/benchmarks_out/benchmark_results_Jul25-1449.pkl',\n",
    "    'loop_connected' : '../save/dev_simulation_oneloop_connected-Jul25/benchmarks_out/benchmark_results_Jul25-1448.pkl',\n",
    "    'tree_full' : '../save/dev_simulation_tree-Jul25/benchmarks_out/benchmark_results_Jul25-1141.pkl',\n",
    "    'tree_partial' : '../save/dev_simulation_tree_partial-Jul25/benchmarks_out/benchmark_results_Jul25-1228.pkl',\n",
    "    'tree_connected' : '../save/dev_simulation_tree_connected-Jul25/benchmarks_out/benchmark_results_Jul25-1238.pkl',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from ../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_1.pkl\n",
      "Loading results from ../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_5.pkl\n",
      "Loading results from ../save/dev_simulation_cluster_imbalancep1-Jul29/benchmarks_out/benchmark_results_Jul29-1352_20.pkl\n",
      "Loading results from ../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_2_batches.pkl\n",
      "Loading results from ../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_10_batches.pkl\n",
      "Loading results from ../save/dev_simulation_cluster_imbalancep2-Jul29/benchmarks_out/benchmark_results_Jul29-2214_30_batches.pkl\n",
      "Loading results from ../save/dev_simulation_trajectory-Jul24/benchmarks_out/benchmark_results_Jul24-2238.pkl\n",
      "Loading results from ../save/dev_simulation_trajectory_partial-Jul24/benchmarks_out/benchmark_results_Jul24-2239.pkl\n",
      "Loading results from ../save/dev_simulation_trajectory_connected-Jul24/benchmarks_out/benchmark_results_Jul24-2301.pkl\n",
      "Loading results from ../save/dev_simulation_trajectory_varybatch-Jul29/benchmarks_out/benchmark_results_Jul29-2018.pkl\n",
      "Loading results from ../save/dev_simulation_oneloop_full-Jul25/benchmarks_out/benchmark_results_Jul25-1450.pkl\n",
      "Loading results from ../save/dev_simulation_oneloop_partial-Jul25/benchmarks_out/benchmark_results_Jul25-1449.pkl\n",
      "Loading results from ../save/dev_simulation_oneloop_connected-Jul25/benchmarks_out/benchmark_results_Jul25-1448.pkl\n",
      "Loading results from ../save/dev_simulation_tree-Jul25/benchmarks_out/benchmark_results_Jul25-1141.pkl\n",
      "Loading results from ../save/dev_simulation_tree_partial-Jul25/benchmarks_out/benchmark_results_Jul25-1228.pkl\n",
      "Loading results from ../save/dev_simulation_tree_connected-Jul25/benchmarks_out/benchmark_results_Jul25-1238.pkl\n"
     ]
    }
   ],
   "source": [
    "# Read in all the results, and get the combined results\n",
    "import pickle\n",
    "bm_results = {}\n",
    "for key, path in result_path.items():\n",
    "    print(f\"Loading results from {path}\")\n",
    "    out = pickle.load(open(path, \"rb\"))\n",
    "    df = out['combined']\n",
    "    df.index.name = \"Method\"      # ← any short label you like\n",
    "    # Drop \"Method\" that is \"no_noise\" or \"wt_noise\"\n",
    "    df = df[~df.index.str.contains(\"no_noise|wt_noise\")]\n",
    "    bm_results[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# For every “cluster” dataset…\n",
    "#   • blank out the whole Topology block  → NaN\n",
    "#   • blank out the Aggregate‑score/Topology column → NaN\n",
    "#   • recompute Aggregate‑score/Average as the mean of the other three groups\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "for ds_name, df in bm_results.items():\n",
    "    if \"cluster\" not in ds_name:          # only touch the cluster datasets\n",
    "        continue\n",
    "\n",
    "    # 1. blank the detailed Topology metrics\n",
    "    topo_mask = df.columns.get_level_values(0) == \"Topology\"\n",
    "    df.loc[:, topo_mask] = np.nan\n",
    "\n",
    "    # 2. blank the aggregate Topology score\n",
    "    if (\"Aggregate score\", \"Topology\") in df.columns:\n",
    "        df[(\"Aggregate score\", \"Topology\")] = np.nan\n",
    "\n",
    "    # 3. recompute the aggregate Average (skip NaNs)\n",
    "    keep_cols = [\n",
    "        col for col in df.columns\n",
    "        if col[0] == \"Aggregate score\"\n",
    "           and col[1] not in (\"Topology\", \"Average\")   # keep the other 3 groups\n",
    "    ]\n",
    "    df[(\"Aggregate score\", \"Average\")] = (\n",
    "        df[keep_cols].mean(axis=1, skipna=True)\n",
    "    )\n",
    "\n",
    "    # (optional) make sure the dtype is float so NaNs stay NaNs\n",
    "    df[(\"Aggregate score\", \"Average\")] = df[(\"Aggregate score\", \"Average\")].astype(float)\n",
    "\n",
    "    # Reorder the rows based on the \"Aggregate score\" column\n",
    "    df = df.sort_values(by=(\"Aggregate score\", \"Average\"), ascending=False)\n",
    "    # store back (not strictly needed; df is already modified in‑place)\n",
    "    bm_results[ds_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Write to excel results for all benchmarkings\n",
    "with pd.ExcelWriter(save_dir/f\"benchmark_results_{file_suffix}.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name, df in bm_results.items():\n",
    "        # Check if sheetname is too long\n",
    "        if len(sheet_name) > 31:\n",
    "            sheet_name = sheet_name[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a single sheet \n",
    "import pandas as pd\n",
    "import xlsxwriter   # imported implicitly by the engine\n",
    "from pathlib import Path\n",
    "\n",
    "out_file = save_dir / f\"benchmark_results_combined_{file_suffix}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(out_file, engine=\"xlsxwriter\") as writer:\n",
    "    workbook  = writer.book\n",
    "    # Create (or grab) a single sheet that everything goes into\n",
    "    worksheet = workbook.add_worksheet(\"AllResults\")\n",
    "    writer.sheets[\"AllResults\"] = worksheet\n",
    "\n",
    "    # Optional – nice formatting helpers\n",
    "    heading_fmt = workbook.add_format({\"bold\": True, \"align\": \"left\"})\n",
    "    num_fmt     = workbook.add_format({\"num_format\": \"0.000\"})   # example\n",
    "\n",
    "    startrow = 0\n",
    "    for name, df in bm_results.items():\n",
    "        # -- 1. heading ----------------------------------------------------\n",
    "        worksheet.write(startrow, 0, name, heading_fmt)\n",
    "\n",
    "        # -- 2. the table --------------------------------------------------\n",
    "        # If you prefer flat column names, uncomment the three lines below\n",
    "        # flat_cols = [\" | \".join(tup).strip()\n",
    "        #              if isinstance(tup, tuple) else tup\n",
    "        #              for tup in df.columns.to_flat_index()]\n",
    "        # df = df.copy(); df.columns = flat_cols\n",
    "\n",
    "        df.to_excel(\n",
    "            writer,\n",
    "            sheet_name=\"AllResults\",\n",
    "            startrow=startrow + 1,   # leave the heading on its own line\n",
    "            startcol=0,\n",
    "            index=True               # keep row index; drop if you don’t need it\n",
    "        )\n",
    "\n",
    "        # (Optional) apply a numeric format to the body of the table\n",
    "        body_rows  = df.shape[0]\n",
    "        header_rows = df.columns.nlevels        # multi‑index depth\n",
    "        first_body = startrow + 1 + header_rows\n",
    "        last_body  = first_body + body_rows - 1\n",
    "        last_col   = df.shape[1]                # +1 if you wrote the index=False\n",
    "\n",
    "        worksheet.set_column(0, last_col, 12, num_fmt)   # width 12, numeric fmt\n",
    "\n",
    "        # -- 3. bump the cursor -------------------------------------------\n",
    "        startrow += header_rows + body_rows + 3   # +2 ⇒ one completely blank line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── 0. preferences --------------------------------------------------------\n",
    "concord_methods = ['concord_hcl', 'concord_knn']\n",
    "other_methods   = [\n",
    "    'contrastive', 'scanorama', 'liger', 'harmony',\n",
    "    'scvi', 'unintegrated'\n",
    "]\n",
    "subcols = [\"Batch correction\", \"Bio conservation\",\n",
    "           \"Geometry\", \"Topology\", \"Average\"]\n",
    "\n",
    "cmap = sns.color_palette(\"viridis_r\", as_cmap=True)\n",
    "cmap.set_bad(\"lightgrey\")          # grey out any remaining NaNs\n",
    "\n",
    "# ── 1. extract the Aggregate‑score sub‑columns ---------------------------\n",
    "score_tables = {sub: [] for sub in subcols}\n",
    "\n",
    "for ds_name, df in bm_results.items():\n",
    "    for sub in subcols:\n",
    "        col_key = (\"Aggregate score\", sub)\n",
    "        if col_key in df.columns:\n",
    "            score_tables[sub].append(df[col_key].rename(ds_name))\n",
    "\n",
    "score_tables = {sub: pd.concat(lst, axis=1)\n",
    "                for sub, lst in score_tables.items() if lst}\n",
    "\n",
    "# ── 2. helper: canonical method order ------------------------------------\n",
    "def sort_methods(index):\n",
    "    pref = concord_methods + other_methods\n",
    "    pref = [m for m in pref if m in index]         # keep only existing ones\n",
    "    the_rest = sorted([m for m in index if m not in pref])\n",
    "    return pref + the_rest\n",
    "\n",
    "# ── 3. plot one heat‑map per sub‑score -----------------------------------\n",
    "for sub, table in score_tables.items():\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # a) rank within each dataset (higher score ⇒ rank 1).\n",
    "    #    `na_option=\"keep\"` leaves NaNs untouched.\n",
    "    # ---------------------------------------------------------------------\n",
    "    ranked = table.rank(axis=0, ascending=False, method=\"min\", na_option=\"keep\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # b) For sub‑scores other than *Topology*, treat any remaining NaN\n",
    "    #    (true missing data) as worst‑rank in that dataset.\n",
    "    # ---------------------------------------------------------------------\n",
    "    if sub != \"Topology\":\n",
    "        for col in ranked:\n",
    "            max_rank = ranked[col].max(skipna=True)\n",
    "            ranked[col] = ranked[col].fillna(max_rank + 1)\n",
    "\n",
    "    # ranked is still float with possible NaNs\n",
    "    # ---------------------------------------------------------------------\n",
    "    # c) reorder rows so CONCORD variants come first\n",
    "    # ---------------------------------------------------------------------\n",
    "    ranked = ranked.reindex(sort_methods(ranked.index))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # d) annotation text: show rank as int, NaN as \"–\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    annot = ranked.applymap(lambda v: str(int(v)) if pd.notna(v) else \"–\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # e) dynamic figure width and plotting\n",
    "    # ---------------------------------------------------------------------\n",
    "    fig_w = 4.3\n",
    "    fig_h = 4.5\n",
    "\n",
    "    with plt.rc_context(rc=custom_rc):\n",
    "        plt.figure(figsize=(fig_w, fig_h))\n",
    "        ax = sns.heatmap(\n",
    "            ranked.T,           # datasets (incl. clusters) on y‑axis\n",
    "            annot=annot.T,\n",
    "            fmt=\"\",\n",
    "            cmap=cmap,\n",
    "            linewidths=1,\n",
    "            linecolor=\"white\",\n",
    "            cbar=False\n",
    "        )\n",
    "        ax.set_title(f\"{sub} – method rank (lower is better)\", pad=8)\n",
    "        ax.set_xlabel(\"Method\")\n",
    "        ax.set_ylabel(\"Dataset\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        fname = sub.lower().replace(\" \", \"_\")\n",
    "        plt.savefig(save_dir / f\"{fname}_rank_{file_suffix}.pdf\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
