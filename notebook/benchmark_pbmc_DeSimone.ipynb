{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86977db1",
   "metadata": {},
   "source": [
    "# Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237ec61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d578d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/concord/lib/python3.10/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import concord as ccd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "from matplotlib import font_manager, rcParams\n",
    "custom_rc = {\n",
    "    'font.family': 'Arial',  # Set the desired font for this plot\n",
    "}\n",
    "\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9816cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jun09-2244'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_name = \"pbmc_Darmanis\"\n",
    "save_dir = f\"../save/{proj_name}-{time.strftime('%b%d')}/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = f\"../data/{proj_name}/\"\n",
    "data_dir = Path(data_dir)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 0\n",
    "ccd.ul.set_seed(seed)\n",
    "\n",
    "file_suffix = f\"{time.strftime('%b%d-%H%M')}\"\n",
    "file_suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee9052",
   "metadata": {},
   "source": [
    "# Download CellxGene AnnData Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##USER Defined Anndata Object Directory\n",
    "##Keep this if you want to use immediately with other notebooks\n",
    "ANNDATA_OBJECT_DIR='../data/pbmc_Darmanis/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f6f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define domain names for cellxgene\n",
    "domain_name = \"cellxgene.cziscience.com\"\n",
    "site_url = f\"https://{domain_name}\"\n",
    "api_url_base = f\"https://api.{domain_name}\"\n",
    "\n",
    "##Define specific collection ID for this study\n",
    "collection_id = \"398e34a9-8736-4b27-a9a7-31a47a67f446\"\n",
    "\n",
    "##Fetch collection\n",
    "collection_path = f\"/curation/v1/collections/{collection_id}\"\n",
    "collection_url = f\"{api_url_base}{collection_path}\"\n",
    "res = requests.get(url=collection_url)\n",
    "res.raise_for_status()\n",
    "res_content = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c78320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading Honeycomb-rep2 to ../results/anndata_objects/Honeycomb-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_FRP-rep1 to ../results/anndata_objects/10X_FRP-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_3-rep1 to ../results/anndata_objects/10X_3-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading BD-rep1 to ../results/anndata_objects/BD-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading harmony_integrated_data to ../results/anndata_objects/harmony_integrated_data_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_5-rep2 to ../results/anndata_objects/10X_5-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_3-rep2 to ../results/anndata_objects/10X_3-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Honeycomb-rep1 to ../results/anndata_objects/Honeycomb-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Scipio-rep2 to ../results/anndata_objects/Scipio-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading BD-rep2 to ../results/anndata_objects/BD-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_FRP-rep2 to ../results/anndata_objects/10X_FRP-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Fluent-rep1 to ../results/anndata_objects/Fluent-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Parse-rep1 to ../results/anndata_objects/Parse-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Scale-rep1 to ../results/anndata_objects/Scale-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Fluent-rep3 to ../results/anndata_objects/Fluent-rep3_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Scipio-rep1 to ../results/anndata_objects/Scipio-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading 10X_5-rep1 to ../results/anndata_objects/10X_5-rep1_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n",
      "\n",
      "Downloading Fluent-rep2 to ../results/anndata_objects/Fluent-rep2_annotated.h5ad ... \n",
      "\u001b[1m\u001b[38;5;10m100.0% downloaded\u001b[0m\n",
      "\n",
      "Done downloading assets\n"
     ]
    }
   ],
   "source": [
    "kits_downloaded = []\n",
    "for dataset in res_content['datasets']:\n",
    "    assets = dataset[\"assets\"]\n",
    "    dataset_id = dataset[\"dataset_id\"]\n",
    "    kit_name = dataset['title']\n",
    "    kits_downloaded.append(kit_name)\n",
    "    for asset in assets:\n",
    "        if asset['filetype'] == 'H5AD':\n",
    "            download_filename = os.path.join(ANNDATA_OBJECT_DIR, f'{kit_name}_annotated.h5ad')\n",
    "            print(f\"\\nDownloading {kit_name} to {download_filename} ... \")\n",
    "            with requests.get(asset[\"url\"], stream=True) as res:\n",
    "                res.raise_for_status()\n",
    "                filesize = int(res.headers[\"Content-Length\"])\n",
    "                with open(download_filename, \"wb\") as df:\n",
    "                    total_bytes_received = 0\n",
    "                    for chunk in res.iter_content(chunk_size=1024 * 1024):\n",
    "                        df.write(chunk)\n",
    "                        total_bytes_received += len(chunk)\n",
    "                        percent_of_total_upload = float(\"{:.1f}\".format(total_bytes_received / filesize * 100))\n",
    "                        color = \"\\033[38;5;10m\" if percent_of_total_upload == 100 else \"\"\n",
    "                        print(f\"\\033[1m{color}{percent_of_total_upload}% downloaded\\033[0m\\r\", end=\"\")\n",
    "    print(\"\\n\\nDone downloading assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b41282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Kit Data: ['10X_3-rep1', '10X_3-rep2', '10X_5-rep1', '10X_5-rep2', '10X_FRP-rep1', '10X_FRP-rep2', 'BD-rep1', 'BD-rep2', 'Fluent-rep1', 'Fluent-rep2', 'Fluent-rep3', 'Honeycomb-rep1', 'Honeycomb-rep2', 'Parse-rep1', 'Scale-rep1', 'Scipio-rep1', 'Scipio-rep2', 'harmony_integrated_data']\n"
     ]
    }
   ],
   "source": [
    "kits_downloaded = sorted(kits_downloaded)\n",
    "print(f'Downloaded Kit Data: {kits_downloaded}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all anndata objects except the harmony_integrated_data_annotated.h5ad\n",
    "anndata_objects = [f for f in Path(ANNDATA_OBJECT_DIR).glob('*.h5ad') if 'harmony_integrated_data_annotated.h5ad' not in str(f)]\n",
    "print(f'Anndata Objects: {anndata_objects}')\n",
    "# concatenate all anndata objects\n",
    "adata_list = [sc.read_h5ad(f) for f in anndata_objects]\n",
    "adata = adata_list[0].concatenate(adata_list[1:], batch_key='dataset', batch_categories=[f.stem for f in anndata_objects])\n",
    "# save the concatenated anndata object\n",
    "adata.write_h5ad(os.path.join(ANNDATA_OBJECT_DIR, 'concatenated_annotated.h5ad'))\n",
    "print(f'Concatenated Anndata Object saved to {ANNDATA_OBJECT_DIR}/concatenated_annotated.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c190f8",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ca9db",
   "metadata": {},
   "source": [
    "downsample the anndata objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b681efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory for the anndata object\n",
    "ANNDATA_OBJECT_DIR = \"../data/pbmc_Darmanis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f550b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample each dataset to 500 cells and concatenate them into a new anndata object\n",
    "downsampled_adata_list = []\n",
    "# concatenate all anndata objects except the downsampled_concatenated_annotated.h5ad\n",
    "anndata_objects = [f for f in Path(ANNDATA_OBJECT_DIR).glob('*.h5ad') if 'downsampled_' not in str(f)]\n",
    "for f in anndata_objects:\n",
    "    adata = sc.read_h5ad(f)\n",
    "    # downsample to 500 cells\n",
    "    if adata.n_obs > 500:\n",
    "        adata = adata[np.random.choice(adata.n_obs, 500, replace=False)]\n",
    "    downsampled_adata_list.append(adata)\n",
    "# concatenate all downsampled anndata objects\n",
    "downsampled_adata = downsampled_adata_list[0].concatenate(downsampled_adata_list[1:], batch_key='dataset', batch_categories=[f.stem for f in anndata_objects])\n",
    "# save the downsampled anndata object\n",
    "downsampled_adata.write_h5ad(os.path.join(ANNDATA_OBJECT_DIR, 'downsampled_9K.h5ad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8e9de",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c715ba",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ac4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the downsampled anndata object\n",
    "adata = sc.read_h5ad('../data/pbmc_Darmanis/downsampled_9K.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace950d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the raw counts to a new layer\n",
    "adata.layers['counts'] = adata.raw.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51deede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_key = 'dataset'\n",
    "state_key = 'cell_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9483b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_keys = [\n",
    "        # \"unintegrated\",\n",
    "        # \"scanorama\", \"liger\", \"harmony\",\n",
    "        # \"scvi\", \"scanvi\",\n",
    "        \"concord\",\n",
    "        \"concord_class\", \n",
    "        \"concord_decoder\", \"contrastive\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff32fda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Model and log files will not be saved.\n",
      "Concord - WARNING - No input feature list provided. It is recommended to first select features using the command `concord.ul.select_features()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS not found. Using sklearn for k-NN computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_intra_knn: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training: 119it [00:04, 27.04it/s, loss=3.57]\n",
      "Epoch 1 Training: 100%|██████████| 119/119 [00:03<00:00, 34.47it/s, loss=3.26]\n",
      "Epoch 2 Training: 100%|██████████| 119/119 [00:03<00:00, 34.93it/s, loss=3.41]\n",
      "Epoch 3 Training: 100%|██████████| 119/119 [00:03<00:00, 35.86it/s, loss=3.36]\n",
      "Epoch 4 Training: 100%|██████████| 119/119 [00:03<00:00, 35.01it/s, loss=3.46]\n",
      "Epoch 5 Training: 100%|██████████| 119/119 [00:04<00:00, 28.89it/s, loss=3.52]\n",
      "Epoch 6 Training: 100%|██████████| 119/119 [00:03<00:00, 31.00it/s, loss=3.35]\n",
      "Epoch 7 Training: 100%|██████████| 119/119 [00:03<00:00, 34.88it/s, loss=3.37]\n",
      "Epoch 8 Training: 100%|██████████| 119/119 [00:03<00:00, 37.86it/s, loss=3.25]\n",
      "Epoch 9 Training: 100%|██████████| 119/119 [00:03<00:00, 38.94it/s, loss=3.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Skipping model/config saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "concord completed in 39.28 sec.\n",
      "Running UMAP on concord...\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Model and log files will not be saved.\n",
      "Concord - WARNING - No input feature list provided. It is recommended to first select features using the command `concord.ul.select_features()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS not found. Using sklearn for k-NN computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_intra_knn: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training: 119it [00:03, 37.74it/s, loss=4.24]\n",
      "Epoch 1 Training: 100%|██████████| 119/119 [00:03<00:00, 37.93it/s, loss=3.71]\n",
      "Epoch 2 Training: 100%|██████████| 119/119 [00:03<00:00, 37.82it/s, loss=3.69]\n",
      "Epoch 3 Training: 100%|██████████| 119/119 [00:03<00:00, 37.74it/s, loss=3.68]\n",
      "Epoch 4 Training: 100%|██████████| 119/119 [00:03<00:00, 37.62it/s, loss=3.72]\n",
      "Epoch 5 Training: 100%|██████████| 119/119 [00:03<00:00, 38.43it/s, loss=3.73]\n",
      "Epoch 6 Training: 100%|██████████| 119/119 [00:03<00:00, 37.98it/s, loss=3.48]\n",
      "Epoch 7 Training: 100%|██████████| 119/119 [00:03<00:00, 38.06it/s, loss=3.41]\n",
      "Epoch 8 Training: 100%|██████████| 119/119 [00:03<00:00, 36.46it/s, loss=3.45]\n",
      "Epoch 9 Training: 100%|██████████| 119/119 [00:03<00:00, 37.68it/s, loss=3.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Skipping model/config saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "concord_class completed in 34.01 sec.\n",
      "Running UMAP on concord_class...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Model and log files will not be saved.\n",
      "Concord - WARNING - No input feature list provided. It is recommended to first select features using the command `concord.ul.select_features()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS not found. Using sklearn for k-NN computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_intra_knn: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training: 119it [00:05, 20.13it/s, loss=4.48]\n",
      "Epoch 1 Training: 100%|██████████| 119/119 [00:05<00:00, 22.26it/s, loss=4.42]\n",
      "Epoch 2 Training: 100%|██████████| 119/119 [00:05<00:00, 20.43it/s, loss=4.41]\n",
      "Epoch 3 Training: 100%|██████████| 119/119 [00:05<00:00, 22.41it/s, loss=4.59]\n",
      "Epoch 4 Training: 100%|██████████| 119/119 [00:06<00:00, 18.53it/s, loss=4.24]\n",
      "Epoch 5 Training: 100%|██████████| 119/119 [00:05<00:00, 21.86it/s, loss=4.69]\n",
      "Epoch 6 Training: 100%|██████████| 119/119 [00:05<00:00, 21.82it/s, loss=4.31]\n",
      "Epoch 7 Training: 100%|██████████| 119/119 [00:05<00:00, 21.64it/s, loss=4.49]\n",
      "Epoch 8 Training: 100%|██████████| 119/119 [00:05<00:00, 21.41it/s, loss=4.31]\n",
      "Epoch 9 Training: 100%|██████████| 119/119 [00:05<00:00, 21.60it/s, loss=4.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Skipping model/config saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "concord_decoder completed in 59.73 sec.\n",
      "Running UMAP on concord_decoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Model and log files will not be saved.\n",
      "Concord - WARNING - No input feature list provided. It is recommended to first select features using the command `concord.ul.select_features()`.\n",
      "Concord - WARNING - domain/batch information not found, all samples will be treated as from single domain/batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS not found. Using sklearn for k-NN computation.\n",
      "You specified p_intra_domain as 0.95 but you only have one domain. Resetting p_intra_domain to 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_intra_knn: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training: 132it [00:03, 34.67it/s, loss=3.48]\n",
      "Epoch 1 Training: 100%|██████████| 132/132 [00:03<00:00, 36.47it/s, loss=3.43]\n",
      "Epoch 2 Training: 100%|██████████| 132/132 [00:03<00:00, 37.55it/s, loss=3.38]\n",
      "Epoch 3 Training: 100%|██████████| 132/132 [00:03<00:00, 36.90it/s, loss=3.29]\n",
      "Epoch 4 Training: 100%|██████████| 132/132 [00:04<00:00, 30.99it/s, loss=3.29]\n",
      "Epoch 5 Training: 100%|██████████| 132/132 [00:04<00:00, 31.35it/s, loss=3.25]\n",
      "Epoch 6 Training: 100%|██████████| 132/132 [00:04<00:00, 31.19it/s, loss=3.38]\n",
      "Epoch 7 Training: 100%|██████████| 132/132 [00:04<00:00, 31.55it/s, loss=3.3] \n",
      "Epoch 8 Training: 100%|██████████| 132/132 [00:04<00:00, 30.32it/s, loss=3.31]\n",
      "Epoch 9 Training: 100%|██████████| 132/132 [00:04<00:00, 28.09it/s, loss=3.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concord - WARNING - save_dir is None. Skipping model/config saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "contrastive completed in 43.79 sec.\n",
      "Running UMAP on contrastive...\n",
      "✅ Selected methods completed.\n"
     ]
    }
   ],
   "source": [
    "time_log, ram_log, vram_log = ccd.ul.run_integration_methods_pipeline(\n",
    "    adata=adata,                          # Your input AnnData object\n",
    "    methods=combined_keys,            # List of methods to run\n",
    "    batch_key=batch_key,                    # Column in adata.obs for batch info\n",
    "    count_layer=\"counts\",                 # Layer name containing raw counts\n",
    "    class_key=state_key,               # Column in adata.obs for class labels (used in SCANVI and CONCORD variants)\n",
    "    latent_dim=30,                        # Latent dimensionality for PCA and embeddings\n",
    "    device='cpu',                        # Or \"cpu\", or \"mps\" for Apple Silicon\n",
    "    return_corrected=False,                   # Whether to store corrected expression matrices\n",
    "    transform_batch=None,                 # Optionally specify a batch to transform to in scVI\n",
    "    seed=42,                              # Random seed for reproducibility\n",
    "    compute_umap=True,                    # Run UMAP for all output embeddings\n",
    "    umap_n_components=2,\n",
    "    umap_n_neighbors=30,\n",
    "    umap_min_dist=0.5,\n",
    "    verbose=True,                        # Print progress messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97bec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the adata object\n",
    "adata.write_h5ad('../data/pbmc_Darmanis/downsampled_9K.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefa6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554df8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
