{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniforge3/envs/concord/lib/python3.12/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import concord as ccd\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_dir = Path('../data/CBCEcombineN2')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "import time\n",
    "from pathlib import Path\n",
    "proj_name = \"CBCEcombineN2\"\n",
    "save_dir = f\"../save/dev_{proj_name}-{time.strftime('%b%d')}/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "file_suffix = f\"{time.strftime('%b%d-%H%M')}\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '../data/celegans_binyamin/N2_outs/concord_celN2_Jun12-1457.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m n2_adata = \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_h5ad\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/celegans_binyamin/N2_outs/concord_celN2_Jun12-1457.h5ad\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m cbce_adata = sc.read_h5ad(\u001b[33m'\u001b[39m\u001b[33m../data/CE_CB/adata_cbce_Jan30-1028.h5ad\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/concord/lib/python3.12/site-packages/anndata/_io/h5ad.py:239\u001b[39m, in \u001b[36mread_h5ad\u001b[39m\u001b[34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[39m\n\u001b[32m    233\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m    235\u001b[39m rdasp = partial(\n\u001b[32m    236\u001b[39m     read_dense_as_sparse, sparse_format=as_sparse_fmt, axis_chunk=chunk_size\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m iospec.encoding_type == \u001b[33m\"\u001b[39m\u001b[33manndata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/concord/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/concord/lib/python3.12/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '../data/celegans_binyamin/N2_outs/concord_celN2_Jun12-1457.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "n2_adata = sc.read_h5ad('../data/celegans_binyamin/N2_outs/concord_celN2_Jun12-1457.h5ad')\n",
    "cbce_adata = sc.read_h5ad('../data/CE_CB/adata_cbce_Jan30-1028.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adata to ../data/CBCEcombineN2/CBCEcombineN2_Jun28-1823.h5ad\n"
     ]
    }
   ],
   "source": [
    "n2_adata.obs['embryo.time'] = n2_adata.obs['raw.embryo.time']\n",
    "n2_adata.obs['batch'] = 'BZ_N2'\n",
    "n2_adata.obs['batch_fine'] = 'BZ_N2'\n",
    "n2_adata.obs['batch_broad'] = 'BZ_N2'\n",
    "n2_adata.obs['species'] = 'C.elegans'\n",
    "n2_adata.obs_names = [f\"{name}-BZ_N2\" for name in n2_adata.obs_names]\n",
    "\n",
    "cbce_adata.obs['batch_fine'] = cbce_adata.obs['batch'].copy()\n",
    "cbce_adata.obs['batch_broad'] = cbce_adata.obs['dataset3'].astype(str).copy()\n",
    "adata = cbce_adata.concatenate(n2_adata, batch_key='lab', batch_categories=['Murray_CBCE','Gartner_BZ'])\n",
    "adata.X = adata.layers[\"counts\"].copy()\n",
    "# Compute basic statistics\n",
    "sc.pp.calculate_qc_metrics(adata, inplace=True)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "adata.obs['batch'] = adata.obs['batch_broad'].copy()\n",
    "sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=10000, subset=False)\n",
    "sc.tl.pca(adata, n_comps=300, svd_solver='arpack', use_highly_variable=True)\n",
    "\n",
    "adata.write_h5ad(data_dir / f\"{proj_name}_{file_suffix}.h5ad\") # Save the adata object with the encoded embeddings\n",
    "print(f\"Saved adata to {data_dir / f'{proj_name}_{file_suffix}.h5ad'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique batches: ['Waterston_300_minutes', 'Waterston_400_minutes', 'Waterston_500_1_minutes', 'Waterston_500_2_minutes', 'Ce_M03D44_300_minutes', ..., 'batch_300', 'batch_360', 'batch_400', 'batch_500', 'BZ_N2']\n",
      "Length: 22\n",
      "Categories (22, object): ['BZ_N2', 'Ce_M03D44_300_minutes', 'Ce_M03D44_500_minutes', 'Ce_ceh9_300_minutes', ..., 'batch_300', 'batch_360', 'batch_400', 'batch_500']\n",
      "Filtered batches: ['Waterston_300_minutes', 'Waterston_400_minutes', 'Waterston_500_1_minutes', 'Waterston_500_2_minutes', 'Murray_b01', 'Murray_b02', 'Murray_r17', 'BZ_N2']\n",
      "adata_celsub shape: (94276, 13405)\n"
     ]
    }
   ],
   "source": [
    "# Save Packer dataset + N2 dataset separately\n",
    "unique_batches = adata.obs['batch_broad'].unique()\n",
    "print(f\"Unique batches: {unique_batches}\")\n",
    "# If the batch name contains Waterston, Murray or BZ_N2, get it\n",
    "filtered_batches = [batch for batch in unique_batches if 'Waterston' in batch or 'Murray' in batch or 'BZ_N2' in batch]\n",
    "print(f\"Filtered batches: {filtered_batches}\")\n",
    "adata_celsub = adata[adata.obs['batch_broad'].isin(filtered_batches)].copy()\n",
    "print(f\"adata_celsub shape: {adata_celsub.shape}\")\n",
    "adata_celsub.write_h5ad(data_dir / f\"adata_celsub_{file_suffix}.h5ad\")\n",
    "print(f\"Saved adata_celsub to {data_dir / f'adata_celsub_{file_suffix}.h5ad'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"../data/CBCEcombineN2/CBCEcombineN2_Jun28-1823.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_name = \"CBCEcombineN2\"\n",
    "file_name = \"CBCEcombineN2\"\n",
    "file_suffix = time.strftime('%b%d-%H%M')\n",
    "seed = 0\n",
    "\n",
    "save_dir = Path(f\"../save/{proj_name}\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = Path(f\"../data/{proj_name}\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "adata = adata[:, adata.var.highly_variable].copy()\n",
    "adata.write_h5ad(data_dir / f\"{file_name}_preprocessed.h5ad\")\n",
    "print(f\"✅ Preprocessed data saved to {data_dir / f'{file_name}_preprocessed.h5ad'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisCello project created at ../data/CBCEcombineN2/viscello_CBCEcombineN2\n"
     ]
    }
   ],
   "source": [
    "ccd.ul.anndata_to_viscello(adata,\n",
    "                        output_dir=data_dir / f\"viscello_{proj_name}\",\n",
    "                        project_name=proj_name,\n",
    "                        organism='cel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(data_dir / f\"{proj_name}_preprocessed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if nan in adata.obs['batch'], if so show the rows with nan\n",
    "if adata.obs['batch'].isna().any():\n",
    "    print(\"Rows with NaN in 'batch':\")\n",
    "    print(adata.obs[adata.obs['batch'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "concord_args = {\n",
    "        'element_mask_prob': 0.4, # Probability of masking features, recommended to be between 0.2 and 0.5\n",
    "        'feature_mask_prob': 0.2, # Probability of masking features, recommended to be between 0.2 and 0.5\n",
    "        'clr_temperature': 0.4, # Temperature for contrastive loss, recommended to be between 0.1 and 0.5\n",
    "        'n_epochs': 15, # Number of epochs for training, adjust as needed\n",
    "        'save_dir': '../'+str(save_dir) # Directory to save the model and results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_scvi.py\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_scvi.sh\n",
      "\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_concord_hcl.py\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_concord_hcl.sh\n",
      "\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_concord_knn.py\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_concord_knn.sh\n",
      "\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_contrastive.py\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_contrastive.sh\n",
      "\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_harmony.py\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_harmony.sh\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', './generate_py_jobs.py', '--proj_name', 'CBCEcombineN2', '--adata_filename', 'CBCEcombineN2_preprocessed.h5ad', '--methods', 'scvi', 'concord_hcl', 'concord_knn', 'contrastive', 'harmony', '--batch_key', 'batch', '--state_key', 'None', '--latent_dim', '300', '--output_dir', '../jobs', '--device', 'auto', '--conda_env', 'concord', '--mem', '32G', '--runtime', '1:00:00', '--concord_kwargs', '{\"element_mask_prob\": 0.4, \"feature_mask_prob\": 0.2, \"clr_temperature\": 0.4, \"n_epochs\": 15, \"save_dir\": \"../../save/CBCEcombineN2\"}', '--mode', 'wynton'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess, json\n",
    "py_methods = [\"scvi\", \"harmony\", \"scanorama\", \"liger\", \"unintegrated\", \"concord_hcl\", \"concord_knn\", \"contrastive\"]\n",
    "py_methods = [\"scvi\", \"concord_hcl\", \"concord_knn\", \"contrastive\", \"harmony\"]\n",
    "output_dir = '../jobs'\n",
    "device = 'auto'\n",
    "#conda_env = 'cellpath'\n",
    "conda_env = 'concord'\n",
    "batch_key = 'batch'\n",
    "state_key = 'None'\n",
    "latent_dim = '300'  # Adjust as needed, but should match the encoder_dims in concord_args\n",
    "subprocess.run([\n",
    "    \"python\", \"./generate_py_jobs.py\",\n",
    "    \"--proj_name\", proj_name,\n",
    "    \"--adata_filename\", f\"{file_name}_preprocessed.h5ad\",\n",
    "    \"--methods\", *py_methods,\n",
    "    \"--batch_key\", batch_key,\n",
    "    \"--state_key\", state_key,\n",
    "    \"--latent_dim\", latent_dim,\n",
    "    \"--output_dir\", output_dir,\n",
    "    \"--device\", device,\n",
    "    \"--conda_env\", conda_env,\n",
    "    \"--mem\", \"32G\",  # Adjust memory as needed\n",
    "    \"--runtime\", \"1:00:00\",\n",
    "    \"--concord_kwargs\", json.dumps(concord_args),\n",
    "    \"--mode\", \"wynton\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌  Run “../jobs/benchmark_CBCEcombineN2/submit_all_CBCEcombineN2.sh” to queue every job.\n"
     ]
    }
   ],
   "source": [
    "proj_folder = Path(output_dir) / f\"benchmark_{proj_name}\"   # ../jobs/benchmark_<proj>\n",
    "proj_folder.mkdir(exist_ok=True)                      # defensive\n",
    "\n",
    "submit_all = proj_folder / f\"submit_all_{proj_name}.sh\"\n",
    "with submit_all.open(\"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(\"# Auto-generated — submits every job for this project\\n\")\n",
    "    f.write(\"# Run from this folder, or let the script cd into it.\\n\\n\")\n",
    "    f.write('cd \"$(dirname \"$0\")\"\\n\\n')          # ensures we’re in the right dir\n",
    "    for sh_file in sorted(proj_folder.glob(f\"benchmark_{proj_name}_*.sh\")):\n",
    "        f.write(f'qsub \"{sh_file.name}\"\\n')\n",
    "\n",
    "submit_all.chmod(0o755)\n",
    "print(f\"📌  Run “{submit_all}” to queue every job.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_seurat_cca.R\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_seurat_cca.sh\n",
      "\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_seurat_rpca.R\n",
      "✅ Generated: benchmark_CBCEcombineN2/benchmark_CBCEcombineN2_seurat_rpca.sh\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', './generate_seurat_script.py', '--proj_name', 'CBCEcombineN2', '--eset_dir', '../../data/CBCEcombineN2/viscello_CBCEcombineN2', '--methods', 'seurat_cca', 'seurat_rpca', '--batch_key', 'batch', '--state_key', 'None', '--latent_dim', '300', '--mem', '250G', '--runtime', '72:00:00', '--output_dir', '../jobs', '--device', 'auto', '--conda_env', 'cellpath'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate script for Seurat\n",
    "import subprocess\n",
    "r_methods = [\"seurat_cca\", \"seurat_rpca\"]\n",
    "output_dir = '../jobs'\n",
    "device = 'auto'\n",
    "conda_env = 'cellpath'\n",
    "batch_key = 'batch'\n",
    "state_key = 'None'\n",
    "latent_dim = '300' \n",
    "subprocess.run([\n",
    "    \"python\", \"./generate_seurat_script.py\",\n",
    "    \"--proj_name\", proj_name,\n",
    "    \"--eset_dir\", '../'+ str(data_dir / f\"viscello_{proj_name}\"),   # <- folder w/ eset.rds\n",
    "    \"--methods\", *r_methods,\n",
    "    \"--batch_key\", batch_key,\n",
    "    \"--state_key\", state_key,\n",
    "    \"--latent_dim\", latent_dim,\n",
    "    \"--mem\", \"250G\",  # Adjust memory as needed\n",
    "    \"--runtime\", \"72:00:00\",\n",
    "    \"--output_dir\", output_dir,\n",
    "    \"--device\", device,\n",
    "    \"--conda_env\", conda_env\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌  Run “../jobs/benchmark_CBCEcombineN2/submit_sequential_CBCEcombineN2.sh” to queue jobs sequentially.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# create submit_sequential_<proj>.sh  (runs each *.py job in order)\n",
    "# ------------------------------------------------------------------\n",
    "sequential_submit = proj_folder / f\"submit_sequential_{proj_name}.sh\"\n",
    "\n",
    "sequential_template = f\"\"\"#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "cd \"$(dirname \"$0\")\"            # work inside this folder (../jobs)\n",
    "shopt -s nullglob\n",
    "\n",
    "for job in benchmark_{proj_name}_*.py; do\n",
    "  base=${{job%.py}}\n",
    "  log=\"${{base}}.log\"\n",
    "\n",
    "  # ───────────────────────────────────────────────────────────────\n",
    "  # skip if a previous run finished successfully\n",
    "  # ───────────────────────────────────────────────────────────────\n",
    "  if [[ -f \"$log\" ]] && grep -q \"finished OK\" \"$log\"; then\n",
    "      echo \">>> SKIP $job  — already completed\"\n",
    "      continue\n",
    "  fi\n",
    "\n",
    "  echo \">>> $job   $(date)\" | tee -a \"$log\"\n",
    "  if python \"$job\" >>\"$log\" 2>&1; then\n",
    "      echo \">>> finished OK\" | tee -a \"$log\"\n",
    "  else\n",
    "      echo \">>> FAILED\"      | tee -a \"$log\"\n",
    "  fi\n",
    "done\n",
    "\"\"\"\n",
    "\n",
    "sequential_submit.write_text(sequential_template)\n",
    "sequential_submit.chmod(0o755)\n",
    "print(f\"📌  Run “{sequential_submit}” to queue jobs sequentially.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
