{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concord as ccd\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from pathlib import Path\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_name = \"cel_packerN2_predict\"\n",
    "file_name = \"cel_packerN2_predict\"\n",
    "file_suffix = time.strftime('%b%d-%H%M')\n",
    "seed = 0\n",
    "ccd.ul.set_seed(seed)\n",
    "save_dir = Path(f\"../save/{proj_name}\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = Path(f\"../data/{proj_name}\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with CONCORD and scVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import scanpy as sc\n",
    "from scvi.model import SCVI\n",
    "\n",
    "def predict_with_scvi(run_dir: Path, adata: sc.AnnData, out_key: str):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_dir : Path\n",
    "        Folder like “…/scvi_0705-0140”. The actual checkpoint is in run_dir / \"scvi_model.pt\".\n",
    "    adata : AnnData\n",
    "        Full dataset to embed.\n",
    "    out_key : str\n",
    "        Where to store the latent representation in adata.obsm.\n",
    "    \"\"\"\n",
    "    run_dir = Path(run_dir).resolve()\n",
    "\n",
    "    inner_dir = run_dir / \"scvi_model.pt\"     # <── directory that holds model.pt\n",
    "    if not (inner_dir / \"model.pt\").is_file():\n",
    "        raise FileNotFoundError(f\"{inner_dir}/model.pt not found\")\n",
    "\n",
    "    # 1️⃣  harmonise genes / categories\n",
    "    SCVI.prepare_query_anndata(adata, str(inner_dir))\n",
    "\n",
    "    # 2️⃣  stitch query cells onto the trained weights\n",
    "    vae_q = SCVI.load_query_data(\n",
    "        adata,\n",
    "        str(inner_dir),        # folder that contains model.pt\n",
    "    )\n",
    "    vae_q.is_trained = True\n",
    "\n",
    "    # 3️⃣  forward pass\n",
    "    adata.obsm[out_key] = vae_q.get_latent_representation()\n",
    "    return adata.obsm[out_key]\n",
    "\n",
    "\n",
    "\n",
    "def predict_with_concord(model_dir: Path, adata, out_key: str):\n",
    "    model = ccd.Concord.load(model_dir=model_dir)\n",
    "    model.predict_adata(adata, output_key=out_key)\n",
    "    return adata.obsm[out_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/cel_packerN2_predict')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../save/cel_packerN2_downsample_ds100\n",
      "\u001b[34mINFO    \u001b[0m File                                                                                                      \n",
      "         \u001b[35m/Users/QZhu/Documents/CONCORD/Concord_benchmark/save/cel_packerN2_downsample_ds100/scvi_0705-0140/scvi_mod\u001b[0m\n",
      "         \u001b[35mel.pt/\u001b[0m\u001b[95mmodel.pt\u001b[0m already downloaded                                                                         \n",
      "\u001b[34mINFO    \u001b[0m Found \u001b[1;36m100.0\u001b[0m% reference vars in query data.                                                                \n",
      "\u001b[34mINFO    \u001b[0m File                                                                                                      \n",
      "         \u001b[35m/Users/QZhu/Documents/CONCORD/Concord_benchmark/save/cel_packerN2_downsample_ds100/scvi_0705-0140/scvi_mod\u001b[0m\n",
      "         \u001b[35mel.pt/\u001b[0m\u001b[95mmodel.pt\u001b[0m already downloaded                                                                         \n",
      "[scvi        ]  ds100  –  FAILED: 'pyro_param_store'\n",
      "concord - INFO - Loading configuration from: ../save/cel_packerN2_downsample_ds100/concord_hcl_0705-0125/config_Jul05-0127.json\n",
      "concord - INFO - Loading model weights from: ../save/cel_packerN2_downsample_ds100/concord_hcl_0705-0125/final_model_Jul05-0127.pt\n",
      "concord - INFO - Encoder input dim: 10000\n",
      "[concord_hcl ]  ds100  –  FAILED: Torch not compiled with CUDA enabled\n",
      "concord - INFO - Loading configuration from: ../save/cel_packerN2_downsample_ds100/concord_knn_0705-0127/config_Jul05-0128.json\n",
      "concord - INFO - Loading model weights from: ../save/cel_packerN2_downsample_ds100/concord_knn_0705-0127/final_model_Jul05-0128.pt\n",
      "concord - INFO - Encoder input dim: 10000\n",
      "[concord_knn ]  ds100  –  FAILED: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "import concord as ccd\n",
    "from scvi.model import SCVI            # make sure scvi-tools is ≥ 1.1\n",
    "from benchmark_utils import latest_run_dir\n",
    "\n",
    "methods = [\"scvi\", \"concord_hcl\", \"concord_knn\"]\n",
    "fractions = [1.0]\n",
    "ds_proj_name = \"cel_packerN2_downsample\"\n",
    "adata = sc.read_h5ad(data_dir / f\"{file_name}_preprocessed.h5ad\")\n",
    "for frac in fractions:\n",
    "    tag        = f\"ds{int(frac * 100)}\"                         # ds100, ds10, …\n",
    "    cur_proj   = f\"{ds_proj_name}_{tag}\"                       # e.g. cel_packerN2_downsample_ds10\n",
    "    proj_save  = Path(f\"../save/\") / cur_proj\n",
    "    print(proj_save)\n",
    "    # fetch latest run directories for each method\n",
    "    paths = {m: latest_run_dir(proj_save, m) for m in methods}\n",
    "    for method, model_path in paths.items():\n",
    "        if model_path is None:\n",
    "            print(f\"[{method:12}]  {tag:<5}  –  no model directory found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        out_key = f\"{method}_{tag}\"\n",
    "        if out_key in adata.obsm:\n",
    "            print(f\"[{method:12}]  {tag:<5}  –  already exists, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if method == \"scvi\":\n",
    "                latent = predict_with_scvi(model_path, adata, out_key)\n",
    "            else:                          # concord_hcl / concord_knn\n",
    "                latent = predict_with_concord(model_path, adata, out_key)\n",
    "\n",
    "            print(f\"[{method:12}]  {tag:<5}  →  stored in .obsm['{out_key}']  \"\n",
    "                  f\"({latent.shape[0]}×{latent.shape[1]})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{method:12}]  {tag:<5}  –  FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered adata to remove bad annotations, new shape: (43686, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "bad_annotation = [np.nan, '', 'unknown', 'None', 'nan', 'NaN', 'NA', 'na', 'unannotated']\n",
    "bad_cells = adata.obs['cell_type'].isin(bad_annotation) \n",
    "adata_ct = adata[~bad_cells].copy()\n",
    "print(f\"✅ Filtered adata to remove bad annotations, new shape: {adata_ct.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AxisArrays with keys: Concord, Concord-decoder, Concord-decoder_UMAP, Concord-decoder_UMAP_3D, Concord_UMAP, Concord_UMAP_3D, X_pca, scvi_ds100, concord_hcl_ds100, concord_knn_ds100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concord.benchmarking.benchmark - INFO - Running Probe benchmark\n",
      "concord.benchmarking.benchmark - INFO - Running linear probe for state with keys ['scvi_ds100', 'concord_hcl_ds100', 'concord_knn_ds100']\n",
      "Detected task: classification\n"
     ]
    }
   ],
   "source": [
    "# Probe only version\n",
    "state_key = 'cell_type'\n",
    "batch_key = 'batch'\n",
    "methods = [\"scvi_ds100\", \"concord_hcl_ds100\", \"concord_knn_ds100\"]\n",
    "out = ccd.bm.run_benchmark_pipeline(\n",
    "    adata_ct,\n",
    "    embedding_keys=methods,\n",
    "    state_key=state_key,\n",
    "    batch_key=batch_key,\n",
    "    save_dir=save_dir / \"benchmarks_celltype_probe\",\n",
    "    file_suffix=file_suffix,  # e.g. \"2025-06-25\"\n",
    "    run=(\"probe\"),          # run only these blocks\n",
    "    plot_individual=False,          # skip the intermediate PDFs\n",
    ")\n",
    "combined_celltype = out[\"combined\"]\n",
    "\n",
    "# Save the benchmark results\n",
    "import pickle\n",
    "with open(save_dir / f\"benchmark_probe_{state_key}_{file_suffix}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "\n",
    "print(f\"✅ Benchmark results saved to: {save_dir / f'benchmark_{state_key}_{file_suffix}.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bad_annotation = [np.nan, '', 'unknown', 'None', 'nan', 'NaN', 'NA', 'na', 'unannotated']\n",
    "\n",
    "state_benchmarks = {}\n",
    "for frac in fractions:  # reverse order to process larger fractions first\n",
    "    adata_name = f\"{file_name}_downsampled_{int(frac * 100)}_final.h5ad\"\n",
    "    tag        = f\"ds{int(frac * 100)}\"                   # keeps job names unique\n",
    "    cur_proj = f\"{proj_name}_{tag}\"\n",
    "    cur_dir = Path(\"../data\") / cur_proj\n",
    "    cur_adata = sc.read_h5ad(cur_dir / adata_name)\n",
    "    bad_cells = cur_adata.obs['lineage_complete'].isin(bad_annotation)\n",
    "\n",
    "    print(f\"Filtering {cur_proj} to remove bad annotations: {bad_cells.sum()} cells out of {len(cur_adata)}\")\n",
    "    adata_ct = cur_adata[~bad_cells].copy()\n",
    "\n",
    "    print(f\"✅ Filtered adata to remove bad annotations, new shape: {adata_ct.shape}\")\n",
    "    state_counts = len(adata_ct.obs['lineage_complete'].value_counts())\n",
    "    batch_counts = len(adata_ct.obs['batch'].value_counts())\n",
    "    print(f\"Cell types: {state_counts}, Batches: {batch_counts}\")\n",
    "    state_key = 'lineage_complete' if state_counts > 1 else None\n",
    "    batch_key = 'batch' if batch_counts > 1 else None\n",
    "    out = ccd.bm.run_benchmark_pipeline(\n",
    "        adata_ct,\n",
    "        embedding_keys=methods,\n",
    "        state_key=state_key,\n",
    "        batch_key=batch_key,\n",
    "        save_dir=save_dir / f\"{cur_proj}_benchmarks_{state_key}\",\n",
    "        file_suffix=file_suffix,  # e.g. \"2025-06-25\"\n",
    "        run=(\"probe\"),          # run only these blocks\n",
    "        plot_individual=False,          # skip the intermediate PDFs\n",
    "    )\n",
    "    combined_celltype = out[\"combined\"]\n",
    "    state_benchmarks[tag] = combined_celltype\n",
    "\n",
    "    # Save the benchmark results\n",
    "    import pickle\n",
    "    with open(save_dir / f\"{cur_proj}_benchmark_{state_key}_{file_suffix}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(out, f)\n",
    "    print(f\"✅ Benchmark results saved to: {save_dir / f'{cur_proj}_benchmark_{state_key}_{file_suffix}.pkl'}\")\n",
    "\n",
    "with open(save_dir / f\"{proj_name}_{state_key}_benchmarks_{file_suffix}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(state_benchmarks, f)\n",
    "print(f\"✅ State benchmarks saved to: {save_dir / f'{proj_name}_{state_key}_benchmarks_{file_suffix}.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
